{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kytk/MagiciansCorner/blob/master/MedNISTClassify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbHU-1U_HE8x"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hide_input": false,
    "id": "0kCscBj8rPbg"
   },
   "source": [
    "# MedNISTデータセットを用いた放射線画像の分類\n",
    "\n",
    "### Bradley J Erickson, MD PhD\n",
    "*Copyright 2019\n",
    "\n",
    "### このNotebookは、Radiology: AI article の以下の論文に対応しています\n",
    "https://pubs.rsna.org/doi/10.1148/ryai.2019190072\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hide_input": true,
    "id": "n0yBAzDzrPbh"
   },
   "source": [
    "このチュートリアルでは以下の3つを行います:\n",
    "\n",
    "1) 6種類の画像をダウンロードし、展開します (頭部CT, 胸部CT, 腹部CT, 頭部MR, 乳腺MR, 胸部Xp) \n",
    "\n",
    "2) 写真を用いて事前にトレーニングされた畳み込みニューラルネットワーク (CNN) と ResNet 34 アーキテクチャを用いて画像を3種類に分類します \n",
    "\n",
    "3) システムの性能を評価し、一番間違っている結果を記録し、どのように性能を改善できるか考慮します \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "hide_input": false,
    "id": "EsCBNolBrPbi",
    "outputId": "c0820272-7787-4f70-cef4-c5a7acab2e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastai in /home/kiyotaka/.local/lib/python3.6/site-packages (2.2.5)\n",
      "Requirement already satisfied: pandas in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (1.0.5)\n",
      "Requirement already satisfied: scipy in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (1.5.4)\n",
      "Requirement already satisfied: fastcore<1.4,>=1.3.8 in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (1.3.19)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastai) (21.0.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from fastai) (3.12)\n",
      "Requirement already satisfied: spacy in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (3.0.1)\n",
      "Requirement already satisfied: torch<1.8,>=1.7.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (1.7.1)\n",
      "Requirement already satisfied: torchvision<0.9,>=0.8 in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (0.8.2)\n",
      "Requirement already satisfied: scikit-learn in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (0.23.1)\n",
      "Requirement already satisfied: packaging in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (20.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from fastai) (2.18.4)\n",
      "Requirement already satisfied: pillow>6.0.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (8.1.0)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastai) (3.2.2)\n",
      "Requirement already satisfied: numpy in /home/kiyotaka/.local/lib/python3.6/site-packages (from fastprogress>=0.2.4->fastai) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /home/kiyotaka/.local/lib/python3.6/site-packages (from torch<1.8,>=1.7.0->fastai) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/kiyotaka/.local/lib/python3.6/site-packages (from torch<1.8,>=1.7.0->fastai) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3/dist-packages (from matplotlib->fastai) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/kiyotaka/.local/lib/python3.6/site-packages (from matplotlib->fastai) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.11.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/kiyotaka/.local/lib/python3.6/site-packages (from pandas->fastai) (2020.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/lib/python3/dist-packages (from scikit-learn->fastai) (0.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from scikit-learn->fastai) (2.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (3.0.1)\n",
      "Requirement already satisfied: pathy in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (0.3.6)\n",
      "Requirement already satisfied: jinja2 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (2.11.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (8.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (4.56.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (0.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (0.3.2)\n",
      "Requirement already satisfied: setuptools in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (47.3.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (2.4.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (1.7.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (3.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/kiyotaka/.local/lib/python3.6/site-packages (from spacy->fastai) (1.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/kiyotaka/.local/lib/python3.6/site-packages (from importlib-metadata>=0.20->spacy->fastai) (3.1.0)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /home/kiyotaka/.local/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.0->spacy->fastai) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /home/kiyotaka/.local/lib/python3.6/site-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.0->spacy->fastai) (0.15)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/kiyotaka/.local/lib/python3.6/site-packages (from typer<0.4.0,>=0.3.0->spacy->fastai) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy->fastai) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/kiyotaka/.local/lib/python3.6/site-packages (from pathy->spacy->fastai) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "# セル 1\n",
    "# 最初に、fastai ライブラリをインストールしたうえで、必要なモジュールをインポートします\n",
    "!pip3 install fastai\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "JWMnwRXA0vfv",
    "outputId": "952335ef-6605-407a-84d4-2bc0b7a90db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-11 23:14:09--  https://docs.google.com/uc?export=download&id=1mqgBKTB0MtGf8Fhc8HaedJyiD8yMoXOh\n",
      "docs.google.com (docs.google.com) をDNSに問いあわせています... 2404:6800:4004:81f::200e, 172.217.175.110\n",
      "docs.google.com (docs.google.com)|2404:6800:4004:81f::200e|:443 に接続しています... 失敗しました: 接続を拒否されました.\n",
      "docs.google.com (docs.google.com)|172.217.175.110|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 302 Moved Temporarily\n",
      "場所: https://doc-0s-60-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/98r88rsveqqso10d08rnbt001goug5ej/1613052825000/16160187475894979440/*/1mqgBKTB0MtGf8Fhc8HaedJyiD8yMoXOh?e=download [続く]\n",
      "警告: HTTPはワイルドカードに対応していません。\n",
      "--2021-02-11 23:14:21--  https://doc-0s-60-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/98r88rsveqqso10d08rnbt001goug5ej/1613052825000/16160187475894979440/*/1mqgBKTB0MtGf8Fhc8HaedJyiD8yMoXOh?e=download\n",
      "doc-0s-60-docs.googleusercontent.com (doc-0s-60-docs.googleusercontent.com) をDNSに問いあわせています... 2404:6800:4004:806::2001, 172.217.24.129\n",
      "doc-0s-60-docs.googleusercontent.com (doc-0s-60-docs.googleusercontent.com)|2404:6800:4004:806::2001|:443 に接続しています... 失敗しました: 接続を拒否されました.\n",
      "doc-0s-60-docs.googleusercontent.com (doc-0s-60-docs.googleusercontent.com)|172.217.24.129|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 2022144 (1.9M) [application/zip]\n",
      "`./MedNIST.zip' に保存中\n",
      "\n",
      "./MedNIST.zip       100%[===================>]   1.93M  1.51MB/s    時間 1.3s    \n",
      "\n",
      "2021-02-11 23:14:23 (1.51 MB/s) - `./MedNIST.zip' へ保存完了 [2022144/2022144]\n",
      "\n",
      "CTAbd  CTChest\tCTHead\tCXR  MRBrain  MRBreast\n"
     ]
    }
   ],
   "source": [
    "# セル 2\n",
    "# 再度セルを実行する時の為に、念の為に以前のデータを削除します\n",
    "!rm -rf MagiciansCorner\n",
    "!rm -rf images\n",
    "\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mqgBKTB0MtGf8Fhc8HaedJyiD8yMoXOh' -O ./MedNIST.zip\n",
    "\n",
    "!mkdir images\n",
    "!cd images; unzip -q \"../MedNIST.zip\" \n",
    "!rm -rf MagiciansCorner\n",
    "# macOSによって生成された不要なファイルを削除します\n",
    "!rm -rf ./images/__MACOSX\n",
    "!ls images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qudSFXxQ7whs",
    "outputId": "4b72c52d-640e-4e84-cb7a-0d7ef85a7320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CXR', 'MRBrain', 'CTChest', 'CTHead', 'CTAbd', 'MRBreast']\n"
     ]
    }
   ],
   "source": [
    "# セル 3\n",
    "import os # ローカル用に追加\n",
    "classes_dir = \"./images\"\n",
    "flist = os.listdir(classes_dir)\n",
    "print (flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0r1wat14rPbr"
   },
   "source": [
    "事前にDICOM画像をグレイスケールのJPEG画像に変換し、サイズを 64x64 にしてあります (もし胸部レントゲン画像を 64x64 以上のサイズのままにしていたら、畳み込みニューラルネットワークの実行に失敗します。）すべての画像が同じサイズでないといけません。次の論文でこのことについてもう少し説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDYOXZsPrPcR"
   },
   "source": [
    "## データを表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Md0NwvtbrPcS",
    "outputId": "8504dca8-3450-4d30-bcf1-947bb0b010df"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aug_transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bd82d4effb0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#        ds_tfms=get_transforms(), size=64, num_workers=4).normalize(imagenet_stats)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m data = ImageDataLoaders.from_folder(classes_dir, train=\".\", valid_pct=0.2,\n\u001b[0;32m----> 8\u001b[0;31m         ds_tfms=aug_transforms(), size=64, num_workers=4).normalize(imagenet_stats)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aug_transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# セル 4\n",
    "import numpy as np #ローカルで使えるように追加\n",
    "from fastai.vision.data import ImageDataLoaders # fastai は v2にあがっている\n",
    "np.random.seed(42)\n",
    "#data = ImageDataBunch.from_folder(classes_dir, train=\".\", valid_pct=0.2,\n",
    "#        ds_tfms=get_transforms(), size=64, num_workers=4).normalize(imagenet_stats)\n",
    "data = ImageDataLoaders.from_folder(classes_dir, train=\".\", valid_pct=0.2,\n",
    "        ds_tfms=aug_transforms(), size=64, num_workers=4).normalize(imagenet_stats)\n",
    "\n",
    "data.classes\n",
    "data.classes, data.c, len(data.train_ds), len(data.valid_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In_a6GdirPcZ"
   },
   "source": [
    "Good! Let's take a look at some of our pictures then.\n",
    "いいですね。それではいくつかの画像を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "9116qi8XrPcg",
    "outputId": "c4db0b97-ae64-4be4-c769-ef6146c1e773"
   },
   "outputs": [],
   "source": [
    "# セル 5\n",
    "data.show_batch(rows=3, figsize=(7,8))\n",
    "\n",
    "def get_img(img_url): return open_image(img_url)\n",
    "\n",
    "# 関数 Function that displays many transformations of an image\n",
    "def plots_of_one_image(img_url, tfms, rows=1, cols=3, width=15, height=5, **kwargs):\n",
    "    img = get_img(img_url)\n",
    "    [img.apply_tfms(tfms, **kwargs).show(ax=ax)\n",
    "         for i,ax in enumerate(plt.subplots(rows,cols,figsize=(width,height))[1].flatten())]\n",
    "tfms = get_transforms(flip_vert=False,                # flip vertical and horizontal\n",
    "                      max_rotate=20.0,                # rotation between -30° and 30°\n",
    "                      max_zoom=1.2)                   # zoom between 1 and 1.2\n",
    "# Uncomment the line below to turn off augmentation (sets the transformations to nothing. Note that you will still see many images, but they are all the same\n",
    "# tfms=[[],[]]\n",
    "\n",
    "# Uncomment these 3 lines to show examples of artificial/augmented images from 1 starting image\n",
    "# note that 00000124.jpg is my randomly selected head CT\n",
    "# all displayed images are variants of that 1 image\n",
    "#plots_of_one_image('./images/MRBrain/00000129.jpg',tfms[0],9,14,11,7, size=64)\n",
    "#plt.subplots_adjust(left=0, bottom=0,wspace=0, hspace=0)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gY-nJzEarPcn"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CUKWKC7yrPco",
    "outputId": "b46df8e0-4bcd-4220-db5e-99e2cd1c2030"
   },
   "outputs": [],
   "source": [
    "# セル 6\n",
    "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
    "#learn = cnn_learner(data, models.resnet50, metrics=error_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "uj9t1CDFrPcs",
    "outputId": "44af328a-d54d-4927-98fa-11e34d0663ab"
   },
   "outputs": [],
   "source": [
    "# セル 7\n",
    "learn.fit_one_cycle(3)\n",
    "learn.save(\"MedNIST-34-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72n4nM0mbyk6"
   },
   "source": [
    "#Evaluation\n",
    "* During the training process, the data is split into 3 parts: training, testing and validation. The training data is used to adjust the weights. The GPU does not have enough RAM to store the entire training set of images, so it is split into 'batches'. When all of the images have been used once for training, then an 'epoch' has passed. Once trained for that epoch, it evaluates how well it has learned using the 'test' data set. The performance on the training set is the train_loss and the performance on the validation set is the valid_loss, and the error_rate is also the percentage of cases wrong in the validation set.\n",
    "* It is common practice that after 'acceptable' performance is achieved on the vclidation set, that the system is tested on the 'test' data, and that is what is considered the 'real' performance.\n",
    "* Note that some use 'test' for what is called validation here, and vice versa.\n",
    "\n",
    "* But sometimes the overall error rate doesn't really tell the story. We might care more about false positives than false negatives, and vica versa. Looking at early results can provide valuable insight into the training process, and how to improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "pGIeWkp3bw1R",
    "outputId": "24e8909f-9859-415d-88a7-197119056a5e"
   },
   "outputs": [],
   "source": [
    "# セル 8\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6o6WLPNelGa"
   },
   "source": [
    "# Looking closer\n",
    "* The confusion matrix shows that there is more confusion between chests and abdomens than with heads. Does that surprise you?\n",
    "* Lets look a little closer at those. FastAI has a nice function that can show you the cases the it did the worst on. Think about that--there are 'errors' but what are the worst errors?\n",
    "* Well, the class assigned to an image is the class that gets the highest score. So the 'worst' would be those where the score for the correct class was lowest. The function 'plot_top_losses' will show the predicted class, the real class, and the score, as well as the image for the N \n",
    "(in our case, 9) worst scored cases.\n",
    "* The second line of code in the cell shows another nice feature of FastAI: to get documentation on any function, just type 'doc(function)' and it will print the documentation for that function. AND it also has a link you can click to then see teh actual source code that implements that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "j04p1iASeZe3",
    "outputId": "c80cceea-967c-400c-a718-bd34bdae2859"
   },
   "outputs": [],
   "source": [
    "# セル 9\n",
    "interp.plot_top_losses(9, figsize=(10,10))\n",
    "doc(interp.plot_top_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAfypUMmf_ea"
   },
   "source": [
    "# What do we see?\n",
    "* Most of the errant classes are slices that contain BOTH lung and abdomen. \n",
    "* This is an important point: Data preparation and curation is critical to getting good results\n",
    "* We can argue about how to handle these cases. The correct answer probably depends on your use case. The point is that without seeing these error cases, fyou might never know what was going wrong...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "-rpC_If1rPc0",
    "outputId": "81c0262b-94b3-43e9-9b80-9547dfe58841"
   },
   "outputs": [],
   "source": [
    "# セル 10\n",
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oplN8lro3Rs-"
   },
   "source": [
    "# Extra credit:\n",
    "* We 'cheated' by starting with a network that was already trained on more than 1,000,000 images. That means the system really only had to learn the specific features of these body parts, but the lower level features like edges and lines were already 'known' to be important to the network.\n",
    "* On the other hand, the 'pretrained' network was trained on photographic images, which are color, not gray scale, and had a matrix size other than 64x64. \n",
    "* While we could start from scratch, a better option might be to use the pre-trained values, but allow any of the weights and kernels in the network to be changed, and that is what 'unfreeze' does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2byn4o2FrPc8",
    "outputId": "0d361539-28c9-412d-b9d5-b7a579af2aae"
   },
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "learn.fit_one_cycle(5, max_lr=slice(3e-6,3e-5))\n",
    "learn.save(\"Unfreeze-34-1\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MedNISTClassify.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
