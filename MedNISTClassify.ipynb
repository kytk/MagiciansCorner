{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kytk/MagiciansCorner/blob/master/MedNISTClassify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbHU-1U_HE8x"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hide_input": false,
    "id": "0kCscBj8rPbg"
   },
   "source": [
    "# MedNISTデータセットを用いた放射線画像の分類\n",
    "\n",
    "### Bradley J Erickson, MD PhD\n",
    "*Copyright 2019\n",
    "\n",
    "### このNotebookは、Radiology: AI article の以下の論文に対応しています\n",
    "https://pubs.rsna.org/doi/10.1148/ryai.2019190072\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hide_input": true,
    "id": "n0yBAzDzrPbh"
   },
   "source": [
    "このチュートリアルでは以下の3つを行います:\n",
    "\n",
    "1) 6種類の画像をダウンロードし、展開します (頭部CT, 胸部CT, 腹部CT, 頭部MR, 乳腺MR, 胸部Xp) \n",
    "\n",
    "2) 写真を用いて事前にトレーニングされた畳み込みニューラルネットワーク (CNN) と ResNet 34 アーキテクチャを用いて画像を3種類に分類します \n",
    "\n",
    "3) システムの性能を評価し、一番間違っている結果を記録し、どのように性能を改善できるか考慮します \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "hide_input": false,
    "id": "EsCBNolBrPbi",
    "outputId": "c0820272-7787-4f70-cef4-c5a7acab2e1d"
   },
   "outputs": [],
   "source": [
    "# セル 1\n",
    "# 最初に、fastai ライブラリをインストールしたうえで、必要なモジュールをインポートします\n",
    "!pip3 install fastai\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "JWMnwRXA0vfv",
    "outputId": "952335ef-6605-407a-84d4-2bc0b7a90db2"
   },
   "outputs": [],
   "source": [
    "# セル 2\n",
    "# 再度セルを実行する時の為に、念の為に以前のデータを削除します\n",
    "!rm -rf MagiciansCorner\n",
    "!rm -rf images\n",
    "\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mqgBKTB0MtGf8Fhc8HaedJyiD8yMoXOh' -O ./MedNIST.zip\n",
    "\n",
    "!mkdir images\n",
    "!cd images; unzip -q \"../MedNIST.zip\" \n",
    "!rm -rf MagiciansCorner\n",
    "# macOSによって生成された不要なファイルを削除します\n",
    "!rm -rf ./images/__MACOSX\n",
    "!ls images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qudSFXxQ7whs",
    "outputId": "4b72c52d-640e-4e84-cb7a-0d7ef85a7320"
   },
   "outputs": [],
   "source": [
    "# セル 3\n",
    "import os # ローカル用に追加\n",
    "classes_dir = \"./images\"\n",
    "flist = os.listdir(classes_dir)\n",
    "print (flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0r1wat14rPbr"
   },
   "source": [
    "事前にDICOM画像をグレイスケールのJPEG画像に変換し、サイズを 64x64 にしてあります (もし胸部レントゲン画像を 64x64 以上のサイズのままにしていたら、畳み込みニューラルネットワークの実行に失敗します。）すべての画像が同じサイズでないといけません。次の論文でこのことについてもう少し説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDYOXZsPrPcR"
   },
   "source": [
    "## データを表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Md0NwvtbrPcS",
    "outputId": "8504dca8-3450-4d30-bcf1-947bb0b010df"
   },
   "outputs": [],
   "source": [
    "# セル 4\n",
    "import numpy as np #ローカルで使えるように追加\n",
    "from fastai.vision.data import ImageDataLoaders # fastai は v2にあがっている\n",
    "np.random.seed(42)\n",
    "#data = ImageDataBunch.from_folder(classes_dir, train=\".\", valid_pct=0.2,\n",
    "#        ds_tfms=get_transforms(), size=64, num_workers=4).normalize(imagenet_stats)\n",
    "data = ImageDataLoaders.from_folder(classes_dir, train=\".\", valid_pct=0.2,\n",
    "        ds_tfms=aug_transforms(), size=64, num_workers=4).normalize(imagenet_stats)\n",
    "\n",
    "data.classes\n",
    "data.classes, data.c, len(data.train_ds), len(data.valid_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In_a6GdirPcZ"
   },
   "source": [
    "Good! Let's take a look at some of our pictures then.\n",
    "いいですね。それではいくつかの画像を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "9116qi8XrPcg",
    "outputId": "c4db0b97-ae64-4be4-c769-ef6146c1e773"
   },
   "outputs": [],
   "source": [
    "# セル 5\n",
    "data.show_batch(rows=3, figsize=(7,8))\n",
    "\n",
    "def get_img(img_url): return open_image(img_url)\n",
    "\n",
    "# 関数 Function that displays many transformations of an image\n",
    "def plots_of_one_image(img_url, tfms, rows=1, cols=3, width=15, height=5, **kwargs):\n",
    "    img = get_img(img_url)\n",
    "    [img.apply_tfms(tfms, **kwargs).show(ax=ax)\n",
    "         for i,ax in enumerate(plt.subplots(rows,cols,figsize=(width,height))[1].flatten())]\n",
    "tfms = get_transforms(flip_vert=False,                # flip vertical and horizontal\n",
    "                      max_rotate=20.0,                # rotation between -30° and 30°\n",
    "                      max_zoom=1.2)                   # zoom between 1 and 1.2\n",
    "# Uncomment the line below to turn off augmentation (sets the transformations to nothing. Note that you will still see many images, but they are all the same\n",
    "# tfms=[[],[]]\n",
    "\n",
    "# Uncomment these 3 lines to show examples of artificial/augmented images from 1 starting image\n",
    "# note that 00000124.jpg is my randomly selected head CT\n",
    "# all displayed images are variants of that 1 image\n",
    "#plots_of_one_image('./images/MRBrain/00000129.jpg',tfms[0],9,14,11,7, size=64)\n",
    "#plt.subplots_adjust(left=0, bottom=0,wspace=0, hspace=0)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gY-nJzEarPcn"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CUKWKC7yrPco",
    "outputId": "b46df8e0-4bcd-4220-db5e-99e2cd1c2030"
   },
   "outputs": [],
   "source": [
    "# セル 6\n",
    "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
    "#learn = cnn_learner(data, models.resnet50, metrics=error_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "uj9t1CDFrPcs",
    "outputId": "44af328a-d54d-4927-98fa-11e34d0663ab"
   },
   "outputs": [],
   "source": [
    "# セル 7\n",
    "learn.fit_one_cycle(3)\n",
    "learn.save(\"MedNIST-34-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72n4nM0mbyk6"
   },
   "source": [
    "#Evaluation\n",
    "* During the training process, the data is split into 3 parts: training, testing and validation. The training data is used to adjust the weights. The GPU does not have enough RAM to store the entire training set of images, so it is split into 'batches'. When all of the images have been used once for training, then an 'epoch' has passed. Once trained for that epoch, it evaluates how well it has learned using the 'test' data set. The performance on the training set is the train_loss and the performance on the validation set is the valid_loss, and the error_rate is also the percentage of cases wrong in the validation set.\n",
    "* It is common practice that after 'acceptable' performance is achieved on the vclidation set, that the system is tested on the 'test' data, and that is what is considered the 'real' performance.\n",
    "* Note that some use 'test' for what is called validation here, and vice versa.\n",
    "\n",
    "* But sometimes the overall error rate doesn't really tell the story. We might care more about false positives than false negatives, and vica versa. Looking at early results can provide valuable insight into the training process, and how to improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "pGIeWkp3bw1R",
    "outputId": "24e8909f-9859-415d-88a7-197119056a5e"
   },
   "outputs": [],
   "source": [
    "# セル 8\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6o6WLPNelGa"
   },
   "source": [
    "# Looking closer\n",
    "* The confusion matrix shows that there is more confusion between chests and abdomens than with heads. Does that surprise you?\n",
    "* Lets look a little closer at those. FastAI has a nice function that can show you the cases the it did the worst on. Think about that--there are 'errors' but what are the worst errors?\n",
    "* Well, the class assigned to an image is the class that gets the highest score. So the 'worst' would be those where the score for the correct class was lowest. The function 'plot_top_losses' will show the predicted class, the real class, and the score, as well as the image for the N \n",
    "(in our case, 9) worst scored cases.\n",
    "* The second line of code in the cell shows another nice feature of FastAI: to get documentation on any function, just type 'doc(function)' and it will print the documentation for that function. AND it also has a link you can click to then see teh actual source code that implements that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "j04p1iASeZe3",
    "outputId": "c80cceea-967c-400c-a718-bd34bdae2859"
   },
   "outputs": [],
   "source": [
    "# セル 9\n",
    "interp.plot_top_losses(9, figsize=(10,10))\n",
    "doc(interp.plot_top_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAfypUMmf_ea"
   },
   "source": [
    "# What do we see?\n",
    "* Most of the errant classes are slices that contain BOTH lung and abdomen. \n",
    "* This is an important point: Data preparation and curation is critical to getting good results\n",
    "* We can argue about how to handle these cases. The correct answer probably depends on your use case. The point is that without seeing these error cases, fyou might never know what was going wrong...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "-rpC_If1rPc0",
    "outputId": "81c0262b-94b3-43e9-9b80-9547dfe58841"
   },
   "outputs": [],
   "source": [
    "# セル 10\n",
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oplN8lro3Rs-"
   },
   "source": [
    "# Extra credit:\n",
    "* We 'cheated' by starting with a network that was already trained on more than 1,000,000 images. That means the system really only had to learn the specific features of these body parts, but the lower level features like edges and lines were already 'known' to be important to the network.\n",
    "* On the other hand, the 'pretrained' network was trained on photographic images, which are color, not gray scale, and had a matrix size other than 64x64. \n",
    "* While we could start from scratch, a better option might be to use the pre-trained values, but allow any of the weights and kernels in the network to be changed, and that is what 'unfreeze' does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2byn4o2FrPc8",
    "outputId": "0d361539-28c9-412d-b9d5-b7a579af2aae"
   },
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "learn.fit_one_cycle(5, max_lr=slice(3e-6,3e-5))\n",
    "learn.save(\"Unfreeze-34-1\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MedNISTClassify.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
